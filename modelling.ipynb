{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "#from hyperopt import fmin, tpe, hp, Trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOrdinalEncoder:\n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "        self.cat_to_int = {}\n",
    "        self.int_to_cat = {}\n",
    "        for i, cat in enumerate(self.categories):\n",
    "            self.cat_to_int[cat] = i\n",
    "            self.int_to_cat[i] = cat\n",
    "\n",
    "    def transform(self, data):\n",
    "        return np.array([self.cat_to_int[cat] if cat in self.cat_to_int else np.nan for cat in data])\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return np.array([self.int_to_cat[int(cat)] for cat in data])\n",
    "\n",
    "def encode_ordinal_columns(df, ordinal_columns, n_classes):\n",
    "    encoders = {}\n",
    "    encoded_df = df.copy()\n",
    "    for col in ordinal_columns:\n",
    "        unique_values = sorted(df[col].dropna().unique())\n",
    "        categories = unique_values + [f\"extra_class_{i}\" for i in range(n_classes - len(unique_values))]\n",
    "        encoder = CustomOrdinalEncoder(categories)\n",
    "        encoded_df[col] = encoder.transform(df[col])\n",
    "        encoders[col] = encoder\n",
    "    return encoded_df, encoders\n",
    "\n",
    "def impute_missing_ordinal_records(df, ordinal_columns, n_classes=5, max_iter=10, random_state=42):\n",
    "    encoded_df, encoders = encode_ordinal_columns(df, ordinal_columns, n_classes)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=max_iter, estimator=RandomForestRegressor(random_state=random_state), random_state=random_state)\n",
    "    imputed_array = imputer.fit_transform(encoded_df)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "    imputed_df[ordinal_columns] = np.round(imputed_df[ordinal_columns])\n",
    "\n",
    "    for col in ordinal_columns:\n",
    "        imputed_df[col] = encoders[col].inverse_transform(imputed_df[col])\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "def encode_non_ordinal_columns(df, non_ordinal_columns):\n",
    "    encoded_df = pd.get_dummies(df, columns=non_ordinal_columns, drop_first=True)\n",
    "    return encoded_df\n",
    "\n",
    "def impute_missing_non_ordinal_records(df, max_iter=10, random_state=42):\n",
    "    imputer = IterativeImputer(max_iter=max_iter, estimator=RandomForestRegressor(random_state=random_state), random_state=random_state)\n",
    "    imputed_array = imputer.fit_transform(df)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "    return imputed_df\n",
    "\n",
    "def impute_most_common(df):\n",
    "    for column in df.columns:\n",
    "        most_common_value = df[column].mode()[0]\n",
    "        df[column].fillna(most_common_value, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv('Surveydata_train.csv', )\n",
    "survey_df_test = pd.read_csv('Surveydata_test.csv')\n",
    "\n",
    "travel_df = pd.read_csv('Traveldata_train.csv')\n",
    "travel_df_test = pd.read_csv('Traveldata_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(survey_df, travel_df, on= 'ID')\n",
    "merged_df_test = pd.merge(survey_df_test, travel_df_test, on= 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = (\n",
    "    merged_df\n",
    "    # 'Seat_comfort', 'Arrival_time_convenient', 'Catering', 'Onboardwifi_service', 'Onboard_entertainment', 'Online_support',\n",
    "    # 'Onlinebooking_Ease', 'Onboard_service', 'Leg_room', 'Checkin_service', 'Cleanliness', 'Online_boarding'\n",
    "    .replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'], [5, 4, 3, 2, 1, 0])\n",
    "    # Platform_location\n",
    "    .replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'], [5, 4, 3, 2, 1, 0])\n",
    "    # Seat_Class\n",
    "    .replace(['Ordinary', 'Green Car'], [0, 1])\n",
    "    # Gender\n",
    "    .replace(['Male', 'Female'], [0, 1])\n",
    "    # CustomerType\n",
    "    .replace(['Disloyal Customer', 'Loyal Customer'], [0, 1])\n",
    "    # TypeTravel\n",
    "    .replace(['Personal Travel', 'Business Travel'], [0, 1])\n",
    "    # Travel_Class\n",
    "    .replace(['Eco', 'Business'], [0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_df = (\n",
    "    merged_df_test\n",
    "    .replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'], [5, 4, 3, 2, 1, 0])\n",
    "    .replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'], [5, 4, 3, 2, 1, 0])\n",
    "    .replace(['Ordinary', 'Green Car'], [0, 1])\n",
    "    .replace(['Male', 'Female'], [0, 1])\n",
    "    .replace(['Disloyal Customer', 'Loyal Customer'], [0, 1])\n",
    "    .replace(['Personal Travel', 'Business Travel'], [0, 1])\n",
    "    .replace(['Eco', 'Business'], [0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df['Age'] = pd.cut(transformed_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])\n",
    "transformed_test_df['Age'] = pd.cut(transformed_test_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns = [\n",
    "    'Seat_Comfort', 'Onboard_Wifi_Service', \n",
    "    'Onboard_Entertainment', 'Online_Support', 'Ease_of_Online_Booking', 'Onboard_Service', \n",
    "    'Legroom', 'Baggage_Handling', 'CheckIn_Service', 'Cleanliness', 'Online_Boarding'\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'Customer_Type', 'Travel_Class'\n",
    "  ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal_imputed = pd.read_csv('Ordinal_Imputed.csv')\n",
    "ordinal_imputed = pd.read_csv('Ordinal_Imputed_Full_BaysianRidge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_non_ordinal_df = encode_non_ordinal_columns(transformed_df[categorical_columns], categorical_columns)\n",
    "# ordinal_imputed['ID'] = transformed_df['ID']\n",
    "# encoded_non_ordinal_df['ID'] = transformed_df['ID']\n",
    "\n",
    "# encoded_df = pd.merge(encoded_non_ordinal_df, ordinal_imputed, on= 'ID')\n",
    "\n",
    "# categorical_imputed = pd.read_csv('Categorical_Imputed.csv')\n",
    "categorical_imputed = pd.read_csv('Categorical_Imputed_Full_BaysianRidge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = encode_non_ordinal_columns(categorical_imputed.copy().drop(['ID'], axis=1), ordinal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94379 entries, 0 to 94378\n",
      "Data columns (total 63 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Unnamed: 0                  94379 non-null  int64  \n",
      " 1   Seat_Class_1                94379 non-null  float64\n",
      " 2   Gender_1.0                  94379 non-null  float64\n",
      " 3   Customer_Type_1.0           94379 non-null  float64\n",
      " 4   Type_Travel_1.0             94379 non-null  float64\n",
      " 5   Travel_Class_1              94379 non-null  float64\n",
      " 6   Arrival_Time_Convenient     94379 non-null  float64\n",
      " 7   Catering                    94379 non-null  float64\n",
      " 8   Platform_Location           94379 non-null  float64\n",
      " 9   Seat_Comfort_1.0            94379 non-null  uint8  \n",
      " 10  Seat_Comfort_2.0            94379 non-null  uint8  \n",
      " 11  Seat_Comfort_3.0            94379 non-null  uint8  \n",
      " 12  Seat_Comfort_4.0            94379 non-null  uint8  \n",
      " 13  Seat_Comfort_5.0            94379 non-null  uint8  \n",
      " 14  Onboard_Wifi_Service_1.0    94379 non-null  uint8  \n",
      " 15  Onboard_Wifi_Service_2.0    94379 non-null  uint8  \n",
      " 16  Onboard_Wifi_Service_3.0    94379 non-null  uint8  \n",
      " 17  Onboard_Wifi_Service_4.0    94379 non-null  uint8  \n",
      " 18  Onboard_Wifi_Service_5.0    94379 non-null  uint8  \n",
      " 19  Onboard_Entertainment_1.0   94379 non-null  uint8  \n",
      " 20  Onboard_Entertainment_2.0   94379 non-null  uint8  \n",
      " 21  Onboard_Entertainment_3.0   94379 non-null  uint8  \n",
      " 22  Onboard_Entertainment_4.0   94379 non-null  uint8  \n",
      " 23  Onboard_Entertainment_5.0   94379 non-null  uint8  \n",
      " 24  Online_Support_1.0          94379 non-null  uint8  \n",
      " 25  Online_Support_2.0          94379 non-null  uint8  \n",
      " 26  Online_Support_3.0          94379 non-null  uint8  \n",
      " 27  Online_Support_4.0          94379 non-null  uint8  \n",
      " 28  Online_Support_5.0          94379 non-null  uint8  \n",
      " 29  Ease_of_Online_Booking_1.0  94379 non-null  uint8  \n",
      " 30  Ease_of_Online_Booking_2.0  94379 non-null  uint8  \n",
      " 31  Ease_of_Online_Booking_3.0  94379 non-null  uint8  \n",
      " 32  Ease_of_Online_Booking_4.0  94379 non-null  uint8  \n",
      " 33  Ease_of_Online_Booking_5.0  94379 non-null  uint8  \n",
      " 34  Onboard_Service_1.0         94379 non-null  uint8  \n",
      " 35  Onboard_Service_2.0         94379 non-null  uint8  \n",
      " 36  Onboard_Service_3.0         94379 non-null  uint8  \n",
      " 37  Onboard_Service_4.0         94379 non-null  uint8  \n",
      " 38  Onboard_Service_5.0         94379 non-null  uint8  \n",
      " 39  Legroom_1.0                 94379 non-null  uint8  \n",
      " 40  Legroom_2.0                 94379 non-null  uint8  \n",
      " 41  Legroom_3.0                 94379 non-null  uint8  \n",
      " 42  Legroom_4.0                 94379 non-null  uint8  \n",
      " 43  Legroom_5.0                 94379 non-null  uint8  \n",
      " 44  Baggage_Handling_2.0        94379 non-null  uint8  \n",
      " 45  Baggage_Handling_3.0        94379 non-null  uint8  \n",
      " 46  Baggage_Handling_4.0        94379 non-null  uint8  \n",
      " 47  Baggage_Handling_5.0        94379 non-null  uint8  \n",
      " 48  CheckIn_Service_1.0         94379 non-null  uint8  \n",
      " 49  CheckIn_Service_2.0         94379 non-null  uint8  \n",
      " 50  CheckIn_Service_3.0         94379 non-null  uint8  \n",
      " 51  CheckIn_Service_4.0         94379 non-null  uint8  \n",
      " 52  CheckIn_Service_5.0         94379 non-null  uint8  \n",
      " 53  Cleanliness_1.0             94379 non-null  uint8  \n",
      " 54  Cleanliness_2.0             94379 non-null  uint8  \n",
      " 55  Cleanliness_3.0             94379 non-null  uint8  \n",
      " 56  Cleanliness_4.0             94379 non-null  uint8  \n",
      " 57  Cleanliness_5.0             94379 non-null  uint8  \n",
      " 58  Online_Boarding_1.0         94379 non-null  uint8  \n",
      " 59  Online_Boarding_2.0         94379 non-null  uint8  \n",
      " 60  Online_Boarding_3.0         94379 non-null  uint8  \n",
      " 61  Online_Boarding_4.0         94379 non-null  uint8  \n",
      " 62  Online_Boarding_5.0         94379 non-null  uint8  \n",
      "dtypes: float64(8), int64(1), uint8(54)\n",
      "memory usage: 11.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_df.info())\n",
    "display(len(final_df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal_test_imputed = pd.read_csv('Ordinal_Test_Imputed.csv')\n",
    "ordinal_test_imputed = pd.read_csv('Ordinal_Test_Imputed_Full_BaysianRidge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_test_non_ordinal_df = encode_non_ordinal_columns(transformed_test_df[categorical_columns], categorical_columns)\n",
    "# ordinal_test_imputed['ID'] = transformed_test_df['ID']\n",
    "# encoded_test_non_ordinal_df['ID'] = transformed_test_df['ID']\n",
    "\n",
    "# encoded_test_df = pd.merge(encoded_test_non_ordinal_df, ordinal_test_imputed, on= 'ID')\n",
    "\n",
    "# categorical_test_imputed = pd.read_csv('Categorical_Test_Imputed.csv')\n",
    "categorical_test_imputed = pd.read_csv('Categorical_Test_Imputed_Full_BaysianRidge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df = (\n",
    "    encode_non_ordinal_columns(categorical_test_imputed.copy().drop(['ID'], axis=1), ordinal_columns)\n",
    ")\n",
    "\n",
    "final_test_df['CheckIn_Service_1.0'] = 0\n",
    "final_test_df['Cleanliness_1.0'] = 0\n",
    "final_test_df['Onboard_Service_1.0'] = 0\n",
    "final_test_df['Online_Support_1.0'] = 0\n",
    "final_test_df['Platform_Location_1.0'] = 0\n",
    "\n",
    "final_test_df = final_test_df[final_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35602 entries, 0 to 35601\n",
      "Data columns (total 63 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Unnamed: 0                  35602 non-null  int64  \n",
      " 1   Seat_Class_1                35602 non-null  float64\n",
      " 2   Gender_1.0                  35602 non-null  float64\n",
      " 3   Customer_Type_1.0           35602 non-null  float64\n",
      " 4   Type_Travel_1.0             35602 non-null  float64\n",
      " 5   Travel_Class_1              35602 non-null  float64\n",
      " 6   Arrival_Time_Convenient     35602 non-null  float64\n",
      " 7   Catering                    35602 non-null  float64\n",
      " 8   Platform_Location           35602 non-null  float64\n",
      " 9   Seat_Comfort_1.0            35602 non-null  uint8  \n",
      " 10  Seat_Comfort_2.0            35602 non-null  uint8  \n",
      " 11  Seat_Comfort_3.0            35602 non-null  uint8  \n",
      " 12  Seat_Comfort_4.0            35602 non-null  uint8  \n",
      " 13  Seat_Comfort_5.0            35602 non-null  uint8  \n",
      " 14  Onboard_Wifi_Service_1.0    35602 non-null  uint8  \n",
      " 15  Onboard_Wifi_Service_2.0    35602 non-null  uint8  \n",
      " 16  Onboard_Wifi_Service_3.0    35602 non-null  uint8  \n",
      " 17  Onboard_Wifi_Service_4.0    35602 non-null  uint8  \n",
      " 18  Onboard_Wifi_Service_5.0    35602 non-null  uint8  \n",
      " 19  Onboard_Entertainment_1.0   35602 non-null  uint8  \n",
      " 20  Onboard_Entertainment_2.0   35602 non-null  uint8  \n",
      " 21  Onboard_Entertainment_3.0   35602 non-null  uint8  \n",
      " 22  Onboard_Entertainment_4.0   35602 non-null  uint8  \n",
      " 23  Onboard_Entertainment_5.0   35602 non-null  uint8  \n",
      " 24  Online_Support_1.0          35602 non-null  int64  \n",
      " 25  Online_Support_2.0          35602 non-null  uint8  \n",
      " 26  Online_Support_3.0          35602 non-null  uint8  \n",
      " 27  Online_Support_4.0          35602 non-null  uint8  \n",
      " 28  Online_Support_5.0          35602 non-null  uint8  \n",
      " 29  Ease_of_Online_Booking_1.0  35602 non-null  uint8  \n",
      " 30  Ease_of_Online_Booking_2.0  35602 non-null  uint8  \n",
      " 31  Ease_of_Online_Booking_3.0  35602 non-null  uint8  \n",
      " 32  Ease_of_Online_Booking_4.0  35602 non-null  uint8  \n",
      " 33  Ease_of_Online_Booking_5.0  35602 non-null  uint8  \n",
      " 34  Onboard_Service_1.0         35602 non-null  int64  \n",
      " 35  Onboard_Service_2.0         35602 non-null  uint8  \n",
      " 36  Onboard_Service_3.0         35602 non-null  uint8  \n",
      " 37  Onboard_Service_4.0         35602 non-null  uint8  \n",
      " 38  Onboard_Service_5.0         35602 non-null  uint8  \n",
      " 39  Legroom_1.0                 35602 non-null  uint8  \n",
      " 40  Legroom_2.0                 35602 non-null  uint8  \n",
      " 41  Legroom_3.0                 35602 non-null  uint8  \n",
      " 42  Legroom_4.0                 35602 non-null  uint8  \n",
      " 43  Legroom_5.0                 35602 non-null  uint8  \n",
      " 44  Baggage_Handling_2.0        35602 non-null  uint8  \n",
      " 45  Baggage_Handling_3.0        35602 non-null  uint8  \n",
      " 46  Baggage_Handling_4.0        35602 non-null  uint8  \n",
      " 47  Baggage_Handling_5.0        35602 non-null  uint8  \n",
      " 48  CheckIn_Service_1.0         35602 non-null  int64  \n",
      " 49  CheckIn_Service_2.0         35602 non-null  uint8  \n",
      " 50  CheckIn_Service_3.0         35602 non-null  uint8  \n",
      " 51  CheckIn_Service_4.0         35602 non-null  uint8  \n",
      " 52  CheckIn_Service_5.0         35602 non-null  uint8  \n",
      " 53  Cleanliness_1.0             35602 non-null  int64  \n",
      " 54  Cleanliness_2.0             35602 non-null  uint8  \n",
      " 55  Cleanliness_3.0             35602 non-null  uint8  \n",
      " 56  Cleanliness_4.0             35602 non-null  uint8  \n",
      " 57  Cleanliness_5.0             35602 non-null  uint8  \n",
      " 58  Online_Boarding_1.0         35602 non-null  uint8  \n",
      " 59  Online_Boarding_2.0         35602 non-null  uint8  \n",
      " 60  Online_Boarding_3.0         35602 non-null  uint8  \n",
      " 61  Online_Boarding_4.0         35602 non-null  uint8  \n",
      " 62  Online_Boarding_5.0         35602 non-null  uint8  \n",
      "dtypes: float64(8), int64(5), uint8(50)\n",
      "memory usage: 5.2 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_test_df.info())\n",
    "display(len(final_test_df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Classifier Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.copy()\n",
    "y = transformed_df['Overall_Experience'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 0.9290\n",
      "HistGradientBoostingClassifier Accuracy: 0.9270\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifiers = [\n",
    "     ('Logistic Regression', LogisticRegression(solver='liblinear')),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('Support Vector Machine', SVC(kernel='linear', C=1)),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('MLP Classifier', MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)),\n",
    "    ('Stochastic Gradient Descent', SGDClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42)),\n",
    "    ('CatBoost', CatBoostClassifier(verbose=0, random_state=42)),\n",
    "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Ridge', RidgeClassifier(random_state=42)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier(random_state=42))\n",
    "\n",
    "]\n",
    "\n",
    "# Iterate through the classifiers, fit, and print accuracy\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Selecting best performing algoritms: CatBoost, XGBoost, MLP Classifier and Random Forest for hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for best performing classifiers with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/f9_8r_cj7cb6ffsgnzdj0yw40000gn/T/ipykernel_47642/2157112064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrf_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Random Forest best parameters: {rf_grid.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(f\"Random Forest best parameters: {rf_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramona/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/ramona/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/ramona/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/ramona/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/ramona/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/f9_8r_cj7cb6ffsgnzdj0yw40000gn/T/ipykernel_47642/3740098224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmlp_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmlp_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"MLP Classifier best parameters: {mlp_grid.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MLP Classifier\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [200, 300, 400]\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(MLPClassifier(random_state=42), mlp_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "mlp_grid.fit(X_train, y_train)\n",
    "print(f\"MLP Classifier best parameters: {mlp_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(XGBClassifier(eval_metric='mlogloss', random_state=42), xgb_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "print(f\"XGBoost best parameters: {xgb_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "cat_params = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'depth': [3, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "cat_grid = GridSearchCV(CatBoostClassifier(verbose=0, random_state=42), cat_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "cat_grid.fit(X_train, y_train)\n",
    "print(f\"CatBoost best parameters: {cat_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingClassifier best parameters: {'min_samples_leaf': 30, 'max_leaf_nodes': 31, 'max_iter': 500, 'max_depth': 30, 'max_bins': 10, 'learning_rate': 0.1, 'l2_regularization': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# HistGradientBoost\n",
    "hist_params = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_iter': [100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    'max_leaf_nodes': [31, 35, 40],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_leaf': [20, 25, 30],\n",
    "    'max_bins': [5, 10, 20, 40],\n",
    "    'l2_regularization': [0.1, 0.001, 0.0001]\n",
    "}\n",
    "hist_grid = RandomizedSearchCV(HistGradientBoostingClassifier(verbose=0, random_state=42), hist_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "hist_grid.fit(X_train, y_train)\n",
    "print(f\"HistGradientBoostingClassifier best parameters: {hist_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/f9_8r_cj7cb6ffsgnzdj0yw40000gn/T/ipykernel_47642/1505757507.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate best models on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbest_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_grid' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate best models on the test set\n",
    "best_rf = rf_grid.best_estimator_\n",
    "best_mlp = mlp_grid.best_estimator_\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "best_cat = cat_grid.best_estimator_\n",
    "best_hist = hist_grid.best_estimator_\n",
    "\n",
    "for name, clf in [('Random Forest', best_rf), ('MLP Classifier', best_mlp), ('XGBoost', best_xgb), ('CatBoost', best_cat), ('HistGradBoost', hist_grid)]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for best performing classifiers with Hyperopt (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/f9_8r_cj7cb6ffsgnzdj0yw40000gn/T/ipykernel_47642/2411468889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Define extensive hyperparameter search spaces for each classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m space_rf = [\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'log2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hp' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the objective function for optimization\n",
    "def objective(args, classifier_name):\n",
    "    if classifier_name == 'RandomForest':\n",
    "        n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf = args\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            max_features=max_features,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'MLPClassifier':\n",
    "        hidden_layer_sizes, alpha, activation, solver, learning_rate = args\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            alpha=alpha,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        n_estimators, learning_rate, max_depth, gamma, subsample, colsample_bytree, tree_method = args\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            tree_method=tree_method,\n",
    "            random_state=42\n",
    "        ) \n",
    "    elif classifier_name == 'CatBoost':\n",
    "        iterations, learning_rate, depth, l2_leaf_reg = args\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=iterations,\n",
    "            learning_rate=learning_rate,\n",
    "            depth=depth,\n",
    "            l2_leaf_reg=l2_leaf_reg,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    elif classifier_name == 'HistGradBoost':\n",
    "        min_samples_leaf, max_leaf_nodes, max_iter, max_depth, max_bins, learning_rate, l2_regularization = args\n",
    "        clf = HistGradientBoostingClassifier(\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            max_iter=max_iter,\n",
    "            max_depth=max_depth,\n",
    "            max_bins=max_bins,\n",
    "            learning_rate=learning_rate,\n",
    "            l2_regularization=l2_regularization,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    score = -np.mean(cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1))\n",
    "    return score\n",
    "\n",
    "# Define extensive hyperparameter search spaces for each classifier\n",
    "space_rf = [\n",
    "    hp.choice('n_estimators', range(10, 201, 10)),\n",
    "    hp.choice('max_depth', list(range(1, 33)) + [None]),\n",
    "    hp.choice('max_features', ['auto', 'sqrt', 'log2', None] + list(np.arange(0.1, 1.1, 0.1))),\n",
    "    hp.choice('min_samples_split', range(2, 21)),\n",
    "    hp.choice('min_samples_leaf', range(1, 21))\n",
    "]\n",
    "\n",
    "space_mlp = [\n",
    "    hp.choice('hidden_layer_sizes', [(i,) for i in range(10, 101, 10)] + [(i, i) for i in range(10, 101, 10)]),\n",
    "    hp.loguniform('alpha', -5, -1),\n",
    "    hp.choice('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "    hp.choice('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "    hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "]\n",
    "\n",
    "space_xgb = [\n",
    "    hp.choice('n_estimators', range(10, 201, 10)),\n",
    "    hp.loguniform('learning_rate', -5, 0),\n",
    "    hp.choice('max_depth', list(range(1, 33))),\n",
    "    hp.loguniform('gamma', -5, 0),\n",
    "    hp.uniform('subsample', 0.1, 1),\n",
    "    hp.uniform('colsample_bytree', 0.1, 1)\n",
    "]\n",
    "\n",
    "space_cat = [\n",
    "    hp.choice('iterations', range(10, 201, 10)),\n",
    "    hp.loguniform('learning_rate', -5, 0),\n",
    "    hp.choice('depth', list(range(1, 17))),\n",
    "    hp.loguniform('l2_leaf_reg', 0, 5)\n",
    "]\n",
    "\n",
    "# Optimize hyperparameters for each classifier\n",
    "for classifier_name, space in [('RandomForest', space_rf), ('MLPClassifier', space_mlp), ('XGBoost', space_xgb), ('CatBoost', space_cat)]:\n",
    "    trials = Trials()\n",
    "    best = fmin(lambda args: objective(args, classifier_name), space, algo=tpe.suggest, max_evals=200, trials=trials)\n",
    "    print(f\"{classifier_name} best parameters: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found out in Google Colab through Hyperopt\n",
    "best_rf = {\n",
    "    'max_depth': 27, \n",
    "    'max_features': 6, \n",
    "    'min_samples_leaf': 1, \n",
    "    'min_samples_split': 13, \n",
    "    'n_estimators': 90\n",
    "    }\n",
    "\n",
    "best_mlp = {\n",
    "    'activation': 'relu',\n",
    "    'alpha': 0.059008349443448974,\n",
    "    'hidden_layer_sizes': (30, 30),\n",
    "    'learning_rate': 'invscaling',\n",
    "    'solver': 'adam'\n",
    "}\n",
    "\n",
    "best_xgb = {\n",
    "    'colsample_bytree': 0.9334254355105551,\n",
    "    'gamma': 0.008801002728149786,\n",
    "    'learning_rate': 0.1257056414560802,\n",
    "    'max_depth': 9,\n",
    "    'n_estimators': 170,\n",
    "    'subsample': 0.9801605605745425,\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "best_cat = {\n",
    "    'l2_leaf_reg': 3.9355845098832787,\n",
    "    'learning_rate': 0.12846062021329857,\n",
    "    'depth': 9,\n",
    "    'iterations': 17\n",
    "}\n",
    "\n",
    "best_hist = {\n",
    "    'min_samples_leaf': 30,\n",
    "    'max_leaf_nodes': 31,\n",
    "    'max_iter': 500,\n",
    "    'max_depth': 30,\n",
    "    'max_bins': 10,\n",
    "    'learning_rate': 0.1 ,\n",
    "    'l2_regularization': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(classifier_name, best_params):\n",
    "    if classifier_name == 'RandomForest':\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'] if best_params['max_depth'] != 32 else None,\n",
    "            max_features=best_params['max_features'],\n",
    "            min_samples_split=best_params['min_samples_split'],\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'MLPClassifier':\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "            alpha=best_params['alpha'],\n",
    "            activation=best_params['activation'],\n",
    "            solver=best_params['solver'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            gamma=best_params['gamma'],\n",
    "            subsample=best_params['subsample'],\n",
    "            colsample_bytree=best_params['colsample_bytree'],\n",
    "            tree_method=best_params['tree_method'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'CatBoost':\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=best_params['iterations'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            depth=best_params['depth'],\n",
    "            l2_leaf_reg=best_params['l2_leaf_reg'],\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    elif classifier_name == 'HistGradBoost':\n",
    "        clf = HistGradientBoostingClassifier(\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            max_leaf_nodes=best_params['max_leaf_nodes'],\n",
    "            max_iter=best_params['max_iter'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            max_bins=best_params['max_bins'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            l2_regularization=best_params['l2_regularization'],\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = final_test_df.copy()\n",
    "\n",
    "X_train = final_df.copy()\n",
    "y_train = transformed_df['Overall_Experience'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best parameters for each classifier in a dictionary\n",
    "best_params_dict = {\n",
    "    #'RandomForest': best_rf,\n",
    "    #'MLPClassifier': best_mlp,\n",
    "    #'XGBoost': best_xgb,\n",
    "    #'CatBoost': best_cat,\n",
    "    'HistGradBoost': best_hist\n",
    "}\n",
    "\n",
    "# Store results\n",
    "result = {}\n",
    "\n",
    "# Evaluate test set accuracy with the best parameters for each classifier\n",
    "for classifier_name in best_params_dict:\n",
    "    # Build the classifier with the best parameters\n",
    "    clf = build_classifier(classifier_name, best_params_dict[classifier_name])\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    result[classifier_name] = y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['CatBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final.to_csv('Submission_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final_hist = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['HistGradBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final_hist.to_csv('Submission_hist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final_xgboost = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['XGBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final_xgboost.to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final_mlp = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['MLPClassifier']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final_mlp.to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final_rf = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['RandomForest']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final_rf.to_csv('Submission_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = pd.DataFrame(data={'ID': range(99900001,99935603), \\\n",
    "                                  'Overall_Experience_rf': result['RandomForest'], \\\n",
    "                                  'Overall_Experience_mlp': result['MLPClassifier'], \\\n",
    "                                  'Overall_Experience_xgb': result['XGBoost'], \\\n",
    "                                  'Overall_Experience_cat': result['CatBoost'], \\\n",
    "                                  'Overall_Experience_hist': result['HistGradBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "\n",
    "result_final.to_csv('Submission_combined.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
