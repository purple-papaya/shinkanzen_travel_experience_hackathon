{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "!pip install hyperopt\n",
    "from hyperopt import fmin, tpe, hp, Trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOrdinalEncoder:\n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "        self.cat_to_int = {}\n",
    "        self.int_to_cat = {}\n",
    "        for i, cat in enumerate(self.categories):\n",
    "            self.cat_to_int[cat] = i\n",
    "            self.int_to_cat[i] = cat\n",
    "\n",
    "    def transform(self, data):\n",
    "        return np.array([self.cat_to_int[cat] if cat in self.cat_to_int else np.nan for cat in data])\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return np.array([self.int_to_cat[int(cat)] for cat in data])\n",
    "\n",
    "def encode_ordinal_columns(df, ordinal_columns, n_classes):\n",
    "    encoders = {}\n",
    "    encoded_df = df.copy()\n",
    "    for col in ordinal_columns:\n",
    "        unique_values = sorted(df[col].dropna().unique())\n",
    "        categories = unique_values + [f\"extra_class_{i}\" for i in range(n_classes - len(unique_values))]\n",
    "        encoder = CustomOrdinalEncoder(categories)\n",
    "        encoded_df[col] = encoder.transform(df[col])\n",
    "        encoders[col] = encoder\n",
    "    return encoded_df, encoders\n",
    "\n",
    "def impute_missing_ordinal_records(df, ordinal_columns, n_classes=5, max_iter=10, random_state=42):\n",
    "    encoded_df, encoders = encode_ordinal_columns(df, ordinal_columns, n_classes)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=max_iter, estimator=RandomForestRegressor(random_state=random_state), random_state=random_state)\n",
    "    imputed_array = imputer.fit_transform(encoded_df)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "    imputed_df[ordinal_columns] = np.round(imputed_df[ordinal_columns])\n",
    "\n",
    "    for col in ordinal_columns:\n",
    "        imputed_df[col] = encoders[col].inverse_transform(imputed_df[col])\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "def encode_non_ordinal_columns(df, non_ordinal_columns):\n",
    "    encoded_df = pd.get_dummies(df, columns=non_ordinal_columns, drop_first=True)\n",
    "    return encoded_df\n",
    "\n",
    "def impute_missing_non_ordinal_records(df, max_iter=10, random_state=42):\n",
    "    imputer = IterativeImputer(max_iter=max_iter, estimator=RandomForestRegressor(random_state=random_state), random_state=random_state)\n",
    "    imputed_array = imputer.fit_transform(df)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "    return imputed_df\n",
    "\n",
    "def impute_most_common(df):\n",
    "    for column in df.columns:\n",
    "        most_common_value = df[column].mode()[0]\n",
    "        df[column].fillna(most_common_value, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv('Surveydata_train.csv', )\n",
    "survey_df_test = pd.read_csv('Surveydata_test.csv')\n",
    "\n",
    "travel_df = pd.read_csv('Traveldata_train.csv')\n",
    "travel_df_test = pd.read_csv('Traveldata_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(survey_df, travel_df, on= 'ID')\n",
    "merged_df_test = pd.merge(survey_df_test, travel_df_test, on= 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = (\n",
    "    merged_df\n",
    "    # 'Seat_comfort', 'Arrival_time_convenient', 'Catering', 'Onboardwifi_service', 'Onboard_entertainment', 'Online_support',\n",
    "    # 'Onlinebooking_Ease', 'Onboard_service', 'Leg_room', 'Checkin_service', 'Cleanliness', 'Online_boarding'\n",
    "    .replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'], [5, 4, 3, 2, 1, 0])\n",
    "    # Platform_location\n",
    "    .replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'], [5, 4, 3, 2, 1, 0])\n",
    "    # Seat_Class\n",
    "    .replace(['Ordinary', 'Green Car'], [0, 1])\n",
    "    # Gender\n",
    "    .replace(['Male', 'Female'], [0, 1])\n",
    "    # CustomerType\n",
    "    .replace(['Disloyal Customer', 'Loyal Customer'], [0, 1])\n",
    "    # TypeTravel\n",
    "    .replace(['Personal Travel', 'Business Travel'], [0, 1])\n",
    "    # Travel_Class\n",
    "    .replace(['Eco', 'Business'], [0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_df = (\n",
    "    merged_df_test\n",
    "    .replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'], [5, 4, 3, 2, 1, 0])\n",
    "    .replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'], [5, 4, 3, 2, 1, 0])\n",
    "    .replace(['Ordinary', 'Green Car'], [0, 1])\n",
    "    .replace(['Male', 'Female'], [0, 1])\n",
    "    .replace(['Disloyal Customer', 'Loyal Customer'], [0, 1])\n",
    "    .replace(['Personal Travel', 'Business Travel'], [0, 1])\n",
    "    .replace(['Eco', 'Business'], [0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df['Age'] = pd.cut(transformed_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])\n",
    "transformed_test_df['Age'] = pd.cut(transformed_test_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns = [\n",
    "    'Seat_Comfort', 'Onboard_Wifi_Service', \n",
    "    'Onboard_Entertainment', 'Online_Support', 'Ease_of_Online_Booking', 'Onboard_Service', \n",
    "    'Legroom', 'Baggage_Handling', 'CheckIn_Service', 'Cleanliness', 'Online_Boarding'\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'Customer_Type', 'Travel_Class'\n",
    "  ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_imputed = pd.read_csv('Ordinal_Imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_non_ordinal_df = encode_non_ordinal_columns(transformed_df[categorical_columns], categorical_columns)\n",
    "# ordinal_imputed['ID'] = transformed_df['ID']\n",
    "# encoded_non_ordinal_df['ID'] = transformed_df['ID']\n",
    "\n",
    "# encoded_df = pd.merge(encoded_non_ordinal_df, ordinal_imputed, on= 'ID')\n",
    "\n",
    "categorical_imputed = pd.read_csv('Categorical_Imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = encode_non_ordinal_columns(categorical_imputed.copy().drop(['ID'], axis=1), ordinal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94379 entries, 0 to 94378\n",
      "Data columns (total 56 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Customer_Type_1.0           94379 non-null  float64\n",
      " 1   Travel_Class_1              94379 non-null  float64\n",
      " 2   Seat_Comfort_1.0            94379 non-null  uint8  \n",
      " 3   Seat_Comfort_2.0            94379 non-null  uint8  \n",
      " 4   Seat_Comfort_3.0            94379 non-null  uint8  \n",
      " 5   Seat_Comfort_4.0            94379 non-null  uint8  \n",
      " 6   Seat_Comfort_5.0            94379 non-null  uint8  \n",
      " 7   Onboard_Wifi_Service_1.0    94379 non-null  uint8  \n",
      " 8   Onboard_Wifi_Service_2.0    94379 non-null  uint8  \n",
      " 9   Onboard_Wifi_Service_3.0    94379 non-null  uint8  \n",
      " 10  Onboard_Wifi_Service_4.0    94379 non-null  uint8  \n",
      " 11  Onboard_Wifi_Service_5.0    94379 non-null  uint8  \n",
      " 12  Onboard_Entertainment_1.0   94379 non-null  uint8  \n",
      " 13  Onboard_Entertainment_2.0   94379 non-null  uint8  \n",
      " 14  Onboard_Entertainment_3.0   94379 non-null  uint8  \n",
      " 15  Onboard_Entertainment_4.0   94379 non-null  uint8  \n",
      " 16  Onboard_Entertainment_5.0   94379 non-null  uint8  \n",
      " 17  Online_Support_1.0          94379 non-null  uint8  \n",
      " 18  Online_Support_2.0          94379 non-null  uint8  \n",
      " 19  Online_Support_3.0          94379 non-null  uint8  \n",
      " 20  Online_Support_4.0          94379 non-null  uint8  \n",
      " 21  Online_Support_5.0          94379 non-null  uint8  \n",
      " 22  Ease_of_Online_Booking_1.0  94379 non-null  uint8  \n",
      " 23  Ease_of_Online_Booking_2.0  94379 non-null  uint8  \n",
      " 24  Ease_of_Online_Booking_3.0  94379 non-null  uint8  \n",
      " 25  Ease_of_Online_Booking_4.0  94379 non-null  uint8  \n",
      " 26  Ease_of_Online_Booking_5.0  94379 non-null  uint8  \n",
      " 27  Onboard_Service_1.0         94379 non-null  uint8  \n",
      " 28  Onboard_Service_2.0         94379 non-null  uint8  \n",
      " 29  Onboard_Service_3.0         94379 non-null  uint8  \n",
      " 30  Onboard_Service_4.0         94379 non-null  uint8  \n",
      " 31  Onboard_Service_5.0         94379 non-null  uint8  \n",
      " 32  Legroom_1.0                 94379 non-null  uint8  \n",
      " 33  Legroom_2.0                 94379 non-null  uint8  \n",
      " 34  Legroom_3.0                 94379 non-null  uint8  \n",
      " 35  Legroom_4.0                 94379 non-null  uint8  \n",
      " 36  Legroom_5.0                 94379 non-null  uint8  \n",
      " 37  Baggage_Handling_2.0        94379 non-null  uint8  \n",
      " 38  Baggage_Handling_3.0        94379 non-null  uint8  \n",
      " 39  Baggage_Handling_4.0        94379 non-null  uint8  \n",
      " 40  Baggage_Handling_5.0        94379 non-null  uint8  \n",
      " 41  CheckIn_Service_1.0         94379 non-null  uint8  \n",
      " 42  CheckIn_Service_2.0         94379 non-null  uint8  \n",
      " 43  CheckIn_Service_3.0         94379 non-null  uint8  \n",
      " 44  CheckIn_Service_4.0         94379 non-null  uint8  \n",
      " 45  CheckIn_Service_5.0         94379 non-null  uint8  \n",
      " 46  Cleanliness_1.0             94379 non-null  uint8  \n",
      " 47  Cleanliness_2.0             94379 non-null  uint8  \n",
      " 48  Cleanliness_3.0             94379 non-null  uint8  \n",
      " 49  Cleanliness_4.0             94379 non-null  uint8  \n",
      " 50  Cleanliness_5.0             94379 non-null  uint8  \n",
      " 51  Online_Boarding_1.0         94379 non-null  uint8  \n",
      " 52  Online_Boarding_2.0         94379 non-null  uint8  \n",
      " 53  Online_Boarding_3.0         94379 non-null  uint8  \n",
      " 54  Online_Boarding_4.0         94379 non-null  uint8  \n",
      " 55  Online_Boarding_5.0         94379 non-null  uint8  \n",
      "dtypes: float64(2), uint8(54)\n",
      "memory usage: 6.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_df.info())\n",
    "display(len(final_df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_test_imputed = pd.read_csv('Ordinal_Test_Imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_test_non_ordinal_df = encode_non_ordinal_columns(transformed_test_df[categorical_columns], categorical_columns)\n",
    "# ordinal_test_imputed['ID'] = transformed_test_df['ID']\n",
    "# encoded_test_non_ordinal_df['ID'] = transformed_test_df['ID']\n",
    "\n",
    "# encoded_test_df = pd.merge(encoded_test_non_ordinal_df, ordinal_test_imputed, on= 'ID')\n",
    "\n",
    "categorical_test_imputed = pd.read_csv('Categorical_Test_Imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df = (\n",
    "    encode_non_ordinal_columns(categorical_test_imputed.copy().drop(['ID'], axis=1), ordinal_columns)\n",
    ")\n",
    "\n",
    "final_test_df['CheckIn_Service_1.0'] = 0\n",
    "final_test_df['Cleanliness_1.0'] = 0\n",
    "final_test_df['Onboard_Service_1.0'] = 0\n",
    "final_test_df['Online_Support_1.0'] = 0\n",
    "final_test_df['Platform_Location_1.0'] = 0\n",
    "\n",
    "final_test_df = final_test_df[final_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35602 entries, 0 to 35601\n",
      "Data columns (total 56 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Customer_Type_1.0           35602 non-null  float64\n",
      " 1   Travel_Class_1              35602 non-null  float64\n",
      " 2   Seat_Comfort_1.0            35602 non-null  uint8  \n",
      " 3   Seat_Comfort_2.0            35602 non-null  uint8  \n",
      " 4   Seat_Comfort_3.0            35602 non-null  uint8  \n",
      " 5   Seat_Comfort_4.0            35602 non-null  uint8  \n",
      " 6   Seat_Comfort_5.0            35602 non-null  uint8  \n",
      " 7   Onboard_Wifi_Service_1.0    35602 non-null  uint8  \n",
      " 8   Onboard_Wifi_Service_2.0    35602 non-null  uint8  \n",
      " 9   Onboard_Wifi_Service_3.0    35602 non-null  uint8  \n",
      " 10  Onboard_Wifi_Service_4.0    35602 non-null  uint8  \n",
      " 11  Onboard_Wifi_Service_5.0    35602 non-null  uint8  \n",
      " 12  Onboard_Entertainment_1.0   35602 non-null  uint8  \n",
      " 13  Onboard_Entertainment_2.0   35602 non-null  uint8  \n",
      " 14  Onboard_Entertainment_3.0   35602 non-null  uint8  \n",
      " 15  Onboard_Entertainment_4.0   35602 non-null  uint8  \n",
      " 16  Onboard_Entertainment_5.0   35602 non-null  uint8  \n",
      " 17  Online_Support_1.0          35602 non-null  int64  \n",
      " 18  Online_Support_2.0          35602 non-null  uint8  \n",
      " 19  Online_Support_3.0          35602 non-null  uint8  \n",
      " 20  Online_Support_4.0          35602 non-null  uint8  \n",
      " 21  Online_Support_5.0          35602 non-null  uint8  \n",
      " 22  Ease_of_Online_Booking_1.0  35602 non-null  uint8  \n",
      " 23  Ease_of_Online_Booking_2.0  35602 non-null  uint8  \n",
      " 24  Ease_of_Online_Booking_3.0  35602 non-null  uint8  \n",
      " 25  Ease_of_Online_Booking_4.0  35602 non-null  uint8  \n",
      " 26  Ease_of_Online_Booking_5.0  35602 non-null  uint8  \n",
      " 27  Onboard_Service_1.0         35602 non-null  int64  \n",
      " 28  Onboard_Service_2.0         35602 non-null  uint8  \n",
      " 29  Onboard_Service_3.0         35602 non-null  uint8  \n",
      " 30  Onboard_Service_4.0         35602 non-null  uint8  \n",
      " 31  Onboard_Service_5.0         35602 non-null  uint8  \n",
      " 32  Legroom_1.0                 35602 non-null  uint8  \n",
      " 33  Legroom_2.0                 35602 non-null  uint8  \n",
      " 34  Legroom_3.0                 35602 non-null  uint8  \n",
      " 35  Legroom_4.0                 35602 non-null  uint8  \n",
      " 36  Legroom_5.0                 35602 non-null  uint8  \n",
      " 37  Baggage_Handling_2.0        35602 non-null  uint8  \n",
      " 38  Baggage_Handling_3.0        35602 non-null  uint8  \n",
      " 39  Baggage_Handling_4.0        35602 non-null  uint8  \n",
      " 40  Baggage_Handling_5.0        35602 non-null  uint8  \n",
      " 41  CheckIn_Service_1.0         35602 non-null  int64  \n",
      " 42  CheckIn_Service_2.0         35602 non-null  uint8  \n",
      " 43  CheckIn_Service_3.0         35602 non-null  uint8  \n",
      " 44  CheckIn_Service_4.0         35602 non-null  uint8  \n",
      " 45  CheckIn_Service_5.0         35602 non-null  uint8  \n",
      " 46  Cleanliness_1.0             35602 non-null  int64  \n",
      " 47  Cleanliness_2.0             35602 non-null  uint8  \n",
      " 48  Cleanliness_3.0             35602 non-null  uint8  \n",
      " 49  Cleanliness_4.0             35602 non-null  uint8  \n",
      " 50  Cleanliness_5.0             35602 non-null  uint8  \n",
      " 51  Online_Boarding_1.0         35602 non-null  uint8  \n",
      " 52  Online_Boarding_2.0         35602 non-null  uint8  \n",
      " 53  Online_Boarding_3.0         35602 non-null  uint8  \n",
      " 54  Online_Boarding_4.0         35602 non-null  uint8  \n",
      " 55  Online_Boarding_5.0         35602 non-null  uint8  \n",
      "dtypes: float64(2), int64(4), uint8(50)\n",
      "memory usage: 3.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_test_df.info())\n",
    "display(len(final_test_df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Classifier Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.copy()\n",
    "y = transformed_df['Overall_Experience'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifiers = [\n",
    "    ('Logistic Regression', LogisticRegression(solver='liblinear')),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('Support Vector Machine', SVC(kernel='linear', C=1)),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('MLP Classifier', MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)),\n",
    "    ('Stochastic Gradient Descent', SGDClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42)),\n",
    "    ('CatBoost', CatBoostClassifier(verbose=0, random_state=42)),\n",
    "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Ridge', RidgeClassifier(random_state=42)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Iterate through the classifiers, fit, and print accuracy\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Selecting best performing algoritms: CatBoost, XGBoost, MLP Classifier and Random Forest for hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for best performing classifiers with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(f\"Random Forest best parameters: {rf_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [200, 300, 400]\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(MLPClassifier(random_state=42), mlp_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "mlp_grid.fit(X_train, y_train)\n",
    "print(f\"MLP Classifier best parameters: {mlp_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(XGBClassifier(eval_metric='mlogloss', random_state=42), xgb_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "print(f\"XGBoost best parameters: {xgb_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "cat_params = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'depth': [3, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "cat_grid = GridSearchCV(CatBoostClassifier(verbose=0, random_state=42), cat_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "cat_grid.fit(X_train, y_train)\n",
    "print(f\"CatBoost best parameters: {cat_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best models on the test set\n",
    "best_rf = rf_grid.best_estimator_\n",
    "best_mlp = mlp_grid.best_estimator_\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "best_cat = cat_grid.best_estimator_\n",
    "\n",
    "for name, clf in [('Random Forest', best_rf), ('MLP Classifier', best_mlp), ('XGBoost', best_xgb), ('CatBoost', best_cat)]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for best performing classifiers with Hyperopt (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for optimization\n",
    "def objective(args, classifier_name):\n",
    "    if classifier_name == 'RandomForest':\n",
    "        n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf = args\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            max_features=max_features,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'MLPClassifier':\n",
    "        hidden_layer_sizes, alpha, activation, solver, learning_rate = args\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            alpha=alpha,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        n_estimators, learning_rate, max_depth, gamma, subsample, colsample_bytree = args\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            random_state=42\n",
    "        )    \n",
    "    elif classifier_name == 'CatBoost':\n",
    "        iterations, learning_rate, depth, l2_leaf_reg = args\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=iterations,\n",
    "            learning_rate=learning_rate,\n",
    "            depth=depth,\n",
    "            l2_leaf_reg=l2_leaf_reg,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    score = -np.mean(cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1))\n",
    "    return score\n",
    "\n",
    "# Define extensive hyperparameter search spaces for each classifier\n",
    "space_rf = [\n",
    "    hp.choice('n_estimators', range(10, 201, 10)),\n",
    "    hp.choice('max_depth', list(range(1, 33)) + [None]),\n",
    "    hp.choice('max_features', ['auto', 'sqrt', 'log2', None] + list(np.arange(0.1, 1.1, 0.1))),\n",
    "    hp.choice('min_samples_split', range(2, 21)),\n",
    "    hp.choice('min_samples_leaf', range(1, 21))\n",
    "]\n",
    "\n",
    "space_mlp = [\n",
    "    hp.choice('hidden_layer_sizes', [(i,) for i in range(10, 101, 10)] + [(i, i) for i in range(10, 101, 10)]),\n",
    "    hp.loguniform('alpha', -5, -1),\n",
    "    hp.choice('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "    hp.choice('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "    hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "]\n",
    "\n",
    "space_xgb = [\n",
    "    hp.choice('n_estimators', range(10, 201, 10)),\n",
    "    hp.loguniform('learning_rate', -5, 0),\n",
    "    hp.choice('max_depth', list(range(1, 33))),\n",
    "    hp.loguniform('gamma', -5, 0),\n",
    "    hp.uniform('subsample', 0.1, 1),\n",
    "    hp.uniform('colsample_bytree', 0.1, 1)\n",
    "]\n",
    "\n",
    "space_cat = [\n",
    "    hp.choice('iterations', range(10, 201, 10)),\n",
    "    hp.loguniform('learning_rate', -5, 0),\n",
    "    hp.choice('depth', list(range(1, 17))),\n",
    "    hp.loguniform('l2_leaf_reg', 0, 5)\n",
    "]\n",
    "\n",
    "# Optimize hyperparameters for each classifier\n",
    "for classifier_name, space in [('RandomForest', space_rf), ('MLPClassifier', space_mlp), ('XGBoost', space_xgb), ('CatBoost', space_cat)]:\n",
    "    trials = Trials()\n",
    "    best = fmin(lambda args: objective(args, classifier_name), space, algo=tpe.suggest, max_evals=200, trials=trials)\n",
    "    print(f\"{classifier_name} best parameters: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found out in Google Colab through Hyperopt\n",
    "best_rf = {\n",
    "    'max_depth': 27, \n",
    "    'max_features': 0.30000000000000004, \n",
    "    'min_samples_leaf': 1, \n",
    "    'min_samples_split': 13, \n",
    "    'n_estimators': 90\n",
    "    }\n",
    "\n",
    "best_mlp = {\n",
    "    'activation': 'relu',\n",
    "    'alpha': 0.059008349443448974,\n",
    "    'hidden_layer_sizes': (30, 30),\n",
    "    'learning_rate': 'invscaling',\n",
    "    'solver': 'adam'\n",
    "}\n",
    "\n",
    "best_xgb = {\n",
    "    'colsample_bytree': 0.9334254355105551,\n",
    "    'gamma': 0.008801002728149786,\n",
    "    'learning_rate': 0.1257056414560802,\n",
    "    'max_depth': 9,\n",
    "    'n_estimators': 170,\n",
    "    'subsample': 0.9801605605745425\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(classifier_name, best_params):\n",
    "    if classifier_name == 'RandomForest':\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'] if best_params['max_depth'] != 32 else None,\n",
    "            max_features=best_params['max_features'],\n",
    "            min_samples_split=best_params['min_samples_split'],\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'MLPClassifier':\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "            alpha=best_params['alpha'],\n",
    "            activation=best_params['activation'],\n",
    "            solver=best_params['solver'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            gamma=best_params['gamma'],\n",
    "            subsample=best_params['subsample'],\n",
    "            colsample_bytree=best_params['colsample_bytree'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'CatBoost':\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=best_params['iterations'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            depth=best_params['depth'],\n",
    "            l2_leaf_reg=best_params['l2_leaf_reg'],\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = final_test_df.copy()\n",
    "\n",
    "X_train = final_df.copy()\n",
    "y_train = transformed_df['Overall_Experience'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best parameters for each classifier in a dictionary\n",
    "best_params_dict = {\n",
    "    'RandomForest': best_rf,\n",
    "    'MLPClassifier': best_mlp,\n",
    "    'XGBoost': best_xgb\n",
    "    # 'CatBoost': best_cat\n",
    "}\n",
    "\n",
    "# Store results\n",
    "result = {}\n",
    "\n",
    "# Evaluate test set accuracy with the best parameters for each classifier\n",
    "for classifier_name in best_params_dict:\n",
    "    # Build the classifier with the best parameters\n",
    "    clf = build_classifier(classifier_name, best_params_dict[classifier_name])\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    result[classifier_name] = y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['XGBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final.to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
