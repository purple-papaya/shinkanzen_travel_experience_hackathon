{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOrdinalEncoder:\n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "        self.cat_to_int = {}\n",
    "        self.int_to_cat = {}\n",
    "        for i, cat in enumerate(self.categories):\n",
    "            self.cat_to_int[cat] = i\n",
    "            self.int_to_cat[i] = cat\n",
    "\n",
    "    def transform(self, data):\n",
    "        return np.array([self.cat_to_int[cat] if cat in self.cat_to_int else np.nan for cat in data])\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return np.array([self.int_to_cat[int(cat)] for cat in data])\n",
    "\n",
    "def encode_ordinal_columns(df, ordinal_columns, n_classes):\n",
    "    encoders = {}\n",
    "    encoded_df = df.copy()\n",
    "    for col in ordinal_columns:\n",
    "        unique_values = sorted(df[col].dropna().unique())\n",
    "        categories = unique_values + [f\"extra_class_{i}\" for i in range(n_classes - len(unique_values))]\n",
    "        encoder = CustomOrdinalEncoder(categories)\n",
    "        encoded_df[col] = encoder.transform(df[col])\n",
    "        encoders[col] = encoder\n",
    "    return encoded_df, encoders\n",
    "\n",
    "def impute_missing_ordinal_records(df, ordinal_columns, n_classes=5, max_iter=10, random_state=42):\n",
    "    encoded_df, encoders = encode_ordinal_columns(df, ordinal_columns, n_classes)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=max_iter, estimator=RandomForestRegressor(random_state=random_state), random_state=random_state)\n",
    "    imputed_array = imputer.fit_transform(encoded_df)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "    imputed_df[ordinal_columns] = np.round(imputed_df[ordinal_columns])\n",
    "\n",
    "    for col in ordinal_columns:\n",
    "        imputed_df[col] = encoders[col].inverse_transform(imputed_df[col])\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "def encode_non_ordinal_columns(df, non_ordinal_columns):\n",
    "    encoded_df = pd.get_dummies(df, columns=non_ordinal_columns, drop_first=True)\n",
    "    return encoded_df\n",
    "\n",
    "def impute_missing_non_ordinal_records(df, max_iter=10, random_state=42):\n",
    "    imputer = IterativeImputer(max_iter=max_iter, estimator=RandomForestRegressor(random_state=random_state), random_state=random_state)\n",
    "    imputed_array = imputer.fit_transform(df)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "    return imputed_df\n",
    "\n",
    "def impute_most_common(df):\n",
    "    for column in df.columns:\n",
    "        most_common_value = df[column].mode()[0]\n",
    "        df[column].fillna(most_common_value, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv('Surveydata_train.csv', )\n",
    "survey_df_test = pd.read_csv('Surveydata_test.csv')\n",
    "\n",
    "travel_df = pd.read_csv('Traveldata_train.csv')\n",
    "travel_df_test = pd.read_csv('Traveldata_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(survey_df, travel_df, on= 'ID')\n",
    "merged_df_test = pd.merge(survey_df_test, travel_df_test, on= 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = (\n",
    "    merged_df\n",
    "    # 'Seat_comfort', 'Arrival_time_convenient', 'Catering', 'Onboardwifi_service', 'Onboard_entertainment', 'Online_support',\n",
    "    # 'Onlinebooking_Ease', 'Onboard_service', 'Leg_room', 'Checkin_service', 'Cleanliness', 'Online_boarding'\n",
    "    .replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'], [5, 4, 3, 2, 1, 0])\n",
    "    # Platform_location\n",
    "    .replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'], [5, 4, 3, 2, 1, 0])\n",
    "    # Seat_Class\n",
    "    .replace(['Ordinary', 'Green Car'], [0, 1])\n",
    "    # Gender\n",
    "    .replace(['Male', 'Female'], [0, 1])\n",
    "    # CustomerType\n",
    "    .replace(['Disloyal Customer', 'Loyal Customer'], [0, 1])\n",
    "    # TypeTravel\n",
    "    .replace(['Personal Travel', 'Business Travel'], [0, 1])\n",
    "    # Travel_Class\n",
    "    .replace(['Eco', 'Business'], [0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_df = (\n",
    "    merged_df_test\n",
    "    .replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'], [5, 4, 3, 2, 1, 0])\n",
    "    .replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'], [5, 4, 3, 2, 1, 0])\n",
    "    .replace(['Ordinary', 'Green Car'], [0, 1])\n",
    "    .replace(['Male', 'Female'], [0, 1])\n",
    "    .replace(['Disloyal Customer', 'Loyal Customer'], [0, 1])\n",
    "    .replace(['Personal Travel', 'Business Travel'], [0, 1])\n",
    "    .replace(['Eco', 'Business'], [0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df['Age'] = pd.cut(transformed_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])\n",
    "transformed_test_df['Age'] = pd.cut(transformed_test_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns = [\n",
    "    'Seat_Comfort', 'Onboard_Wifi_Service', \n",
    "    'Onboard_Entertainment', 'Online_Support', 'Ease_of_Online_Booking', 'Onboard_Service', \n",
    "    'Legroom', 'Baggage_Handling', 'CheckIn_Service', 'Cleanliness', 'Online_Boarding'\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'Customer_Type', 'Travel_Class'\n",
    "  ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_imputed = pd.read_csv('Ordinal_Imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_non_ordinal_df = encode_non_ordinal_columns(transformed_df[categorical_columns], categorical_columns)\n",
    "# ordinal_imputed['ID'] = transformed_df['ID']\n",
    "# encoded_non_ordinal_df['ID'] = transformed_df['ID']\n",
    "\n",
    "# encoded_df = pd.merge(encoded_non_ordinal_df, ordinal_imputed, on= 'ID')\n",
    "\n",
    "categorical_imputed = pd.read_csv('Categorical_Imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = encode_non_ordinal_columns(categorical_imputed.copy().drop(['ID'], axis=1), ordinal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94379 entries, 0 to 94378\n",
      "Data columns (total 56 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Customer_Type_1.0           94379 non-null  float64\n",
      " 1   Travel_Class_1              94379 non-null  float64\n",
      " 2   Seat_Comfort_1.0            94379 non-null  uint8  \n",
      " 3   Seat_Comfort_2.0            94379 non-null  uint8  \n",
      " 4   Seat_Comfort_3.0            94379 non-null  uint8  \n",
      " 5   Seat_Comfort_4.0            94379 non-null  uint8  \n",
      " 6   Seat_Comfort_5.0            94379 non-null  uint8  \n",
      " 7   Onboard_Wifi_Service_1.0    94379 non-null  uint8  \n",
      " 8   Onboard_Wifi_Service_2.0    94379 non-null  uint8  \n",
      " 9   Onboard_Wifi_Service_3.0    94379 non-null  uint8  \n",
      " 10  Onboard_Wifi_Service_4.0    94379 non-null  uint8  \n",
      " 11  Onboard_Wifi_Service_5.0    94379 non-null  uint8  \n",
      " 12  Onboard_Entertainment_1.0   94379 non-null  uint8  \n",
      " 13  Onboard_Entertainment_2.0   94379 non-null  uint8  \n",
      " 14  Onboard_Entertainment_3.0   94379 non-null  uint8  \n",
      " 15  Onboard_Entertainment_4.0   94379 non-null  uint8  \n",
      " 16  Onboard_Entertainment_5.0   94379 non-null  uint8  \n",
      " 17  Online_Support_1.0          94379 non-null  uint8  \n",
      " 18  Online_Support_2.0          94379 non-null  uint8  \n",
      " 19  Online_Support_3.0          94379 non-null  uint8  \n",
      " 20  Online_Support_4.0          94379 non-null  uint8  \n",
      " 21  Online_Support_5.0          94379 non-null  uint8  \n",
      " 22  Ease_of_Online_Booking_1.0  94379 non-null  uint8  \n",
      " 23  Ease_of_Online_Booking_2.0  94379 non-null  uint8  \n",
      " 24  Ease_of_Online_Booking_3.0  94379 non-null  uint8  \n",
      " 25  Ease_of_Online_Booking_4.0  94379 non-null  uint8  \n",
      " 26  Ease_of_Online_Booking_5.0  94379 non-null  uint8  \n",
      " 27  Onboard_Service_1.0         94379 non-null  uint8  \n",
      " 28  Onboard_Service_2.0         94379 non-null  uint8  \n",
      " 29  Onboard_Service_3.0         94379 non-null  uint8  \n",
      " 30  Onboard_Service_4.0         94379 non-null  uint8  \n",
      " 31  Onboard_Service_5.0         94379 non-null  uint8  \n",
      " 32  Legroom_1.0                 94379 non-null  uint8  \n",
      " 33  Legroom_2.0                 94379 non-null  uint8  \n",
      " 34  Legroom_3.0                 94379 non-null  uint8  \n",
      " 35  Legroom_4.0                 94379 non-null  uint8  \n",
      " 36  Legroom_5.0                 94379 non-null  uint8  \n",
      " 37  Baggage_Handling_2.0        94379 non-null  uint8  \n",
      " 38  Baggage_Handling_3.0        94379 non-null  uint8  \n",
      " 39  Baggage_Handling_4.0        94379 non-null  uint8  \n",
      " 40  Baggage_Handling_5.0        94379 non-null  uint8  \n",
      " 41  CheckIn_Service_1.0         94379 non-null  uint8  \n",
      " 42  CheckIn_Service_2.0         94379 non-null  uint8  \n",
      " 43  CheckIn_Service_3.0         94379 non-null  uint8  \n",
      " 44  CheckIn_Service_4.0         94379 non-null  uint8  \n",
      " 45  CheckIn_Service_5.0         94379 non-null  uint8  \n",
      " 46  Cleanliness_1.0             94379 non-null  uint8  \n",
      " 47  Cleanliness_2.0             94379 non-null  uint8  \n",
      " 48  Cleanliness_3.0             94379 non-null  uint8  \n",
      " 49  Cleanliness_4.0             94379 non-null  uint8  \n",
      " 50  Cleanliness_5.0             94379 non-null  uint8  \n",
      " 51  Online_Boarding_1.0         94379 non-null  uint8  \n",
      " 52  Online_Boarding_2.0         94379 non-null  uint8  \n",
      " 53  Online_Boarding_3.0         94379 non-null  uint8  \n",
      " 54  Online_Boarding_4.0         94379 non-null  uint8  \n",
      " 55  Online_Boarding_5.0         94379 non-null  uint8  \n",
      "dtypes: float64(2), uint8(54)\n",
      "memory usage: 6.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_df.info())\n",
    "display(len(final_df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_test_imputed = pd.read_csv('Ordinal_Test_Imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_test_non_ordinal_df = encode_non_ordinal_columns(transformed_test_df[categorical_columns], categorical_columns)\n",
    "# ordinal_test_imputed['ID'] = transformed_test_df['ID']\n",
    "# encoded_test_non_ordinal_df['ID'] = transformed_test_df['ID']\n",
    "\n",
    "# encoded_test_df = pd.merge(encoded_test_non_ordinal_df, ordinal_test_imputed, on= 'ID')\n",
    "\n",
    "categorical_test_imputed = pd.read_csv('Categorical_Test_Imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df = (\n",
    "    encode_non_ordinal_columns(categorical_test_imputed.copy().drop(['ID'], axis=1), ordinal_columns)\n",
    ")\n",
    "\n",
    "final_test_df['CheckIn_Service_1.0'] = 0\n",
    "final_test_df['Cleanliness_1.0'] = 0\n",
    "final_test_df['Onboard_Service_1.0'] = 0\n",
    "final_test_df['Online_Support_1.0'] = 0\n",
    "final_test_df['Platform_Location_1.0'] = 0\n",
    "\n",
    "final_test_df = final_test_df[final_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35602 entries, 0 to 35601\n",
      "Data columns (total 56 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Customer_Type_1.0           35602 non-null  float64\n",
      " 1   Travel_Class_1              35602 non-null  float64\n",
      " 2   Seat_Comfort_1.0            35602 non-null  uint8  \n",
      " 3   Seat_Comfort_2.0            35602 non-null  uint8  \n",
      " 4   Seat_Comfort_3.0            35602 non-null  uint8  \n",
      " 5   Seat_Comfort_4.0            35602 non-null  uint8  \n",
      " 6   Seat_Comfort_5.0            35602 non-null  uint8  \n",
      " 7   Onboard_Wifi_Service_1.0    35602 non-null  uint8  \n",
      " 8   Onboard_Wifi_Service_2.0    35602 non-null  uint8  \n",
      " 9   Onboard_Wifi_Service_3.0    35602 non-null  uint8  \n",
      " 10  Onboard_Wifi_Service_4.0    35602 non-null  uint8  \n",
      " 11  Onboard_Wifi_Service_5.0    35602 non-null  uint8  \n",
      " 12  Onboard_Entertainment_1.0   35602 non-null  uint8  \n",
      " 13  Onboard_Entertainment_2.0   35602 non-null  uint8  \n",
      " 14  Onboard_Entertainment_3.0   35602 non-null  uint8  \n",
      " 15  Onboard_Entertainment_4.0   35602 non-null  uint8  \n",
      " 16  Onboard_Entertainment_5.0   35602 non-null  uint8  \n",
      " 17  Online_Support_1.0          35602 non-null  int64  \n",
      " 18  Online_Support_2.0          35602 non-null  uint8  \n",
      " 19  Online_Support_3.0          35602 non-null  uint8  \n",
      " 20  Online_Support_4.0          35602 non-null  uint8  \n",
      " 21  Online_Support_5.0          35602 non-null  uint8  \n",
      " 22  Ease_of_Online_Booking_1.0  35602 non-null  uint8  \n",
      " 23  Ease_of_Online_Booking_2.0  35602 non-null  uint8  \n",
      " 24  Ease_of_Online_Booking_3.0  35602 non-null  uint8  \n",
      " 25  Ease_of_Online_Booking_4.0  35602 non-null  uint8  \n",
      " 26  Ease_of_Online_Booking_5.0  35602 non-null  uint8  \n",
      " 27  Onboard_Service_1.0         35602 non-null  int64  \n",
      " 28  Onboard_Service_2.0         35602 non-null  uint8  \n",
      " 29  Onboard_Service_3.0         35602 non-null  uint8  \n",
      " 30  Onboard_Service_4.0         35602 non-null  uint8  \n",
      " 31  Onboard_Service_5.0         35602 non-null  uint8  \n",
      " 32  Legroom_1.0                 35602 non-null  uint8  \n",
      " 33  Legroom_2.0                 35602 non-null  uint8  \n",
      " 34  Legroom_3.0                 35602 non-null  uint8  \n",
      " 35  Legroom_4.0                 35602 non-null  uint8  \n",
      " 36  Legroom_5.0                 35602 non-null  uint8  \n",
      " 37  Baggage_Handling_2.0        35602 non-null  uint8  \n",
      " 38  Baggage_Handling_3.0        35602 non-null  uint8  \n",
      " 39  Baggage_Handling_4.0        35602 non-null  uint8  \n",
      " 40  Baggage_Handling_5.0        35602 non-null  uint8  \n",
      " 41  CheckIn_Service_1.0         35602 non-null  int64  \n",
      " 42  CheckIn_Service_2.0         35602 non-null  uint8  \n",
      " 43  CheckIn_Service_3.0         35602 non-null  uint8  \n",
      " 44  CheckIn_Service_4.0         35602 non-null  uint8  \n",
      " 45  CheckIn_Service_5.0         35602 non-null  uint8  \n",
      " 46  Cleanliness_1.0             35602 non-null  int64  \n",
      " 47  Cleanliness_2.0             35602 non-null  uint8  \n",
      " 48  Cleanliness_3.0             35602 non-null  uint8  \n",
      " 49  Cleanliness_4.0             35602 non-null  uint8  \n",
      " 50  Cleanliness_5.0             35602 non-null  uint8  \n",
      " 51  Online_Boarding_1.0         35602 non-null  uint8  \n",
      " 52  Online_Boarding_2.0         35602 non-null  uint8  \n",
      " 53  Online_Boarding_3.0         35602 non-null  uint8  \n",
      " 54  Online_Boarding_4.0         35602 non-null  uint8  \n",
      " 55  Online_Boarding_5.0         35602 non-null  uint8  \n",
      "dtypes: float64(2), int64(4), uint8(50)\n",
      "memory usage: 3.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_test_df.info())\n",
    "display(len(final_test_df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Classifier Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.copy()\n",
    "y = transformed_df['Overall_Experience'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifiers = [\n",
    "    ('Logistic Regression', LogisticRegression(solver='liblinear')),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('Support Vector Machine', SVC(kernel='linear', C=1)),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('MLP Classifier', MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)),\n",
    "    ('Stochastic Gradient Descent', SGDClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42)),\n",
    "    ('CatBoost', CatBoostClassifier(verbose=0, random_state=42)),\n",
    "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Ridge', RidgeClassifier(random_state=42)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Iterate through the classifiers, fit, and print accuracy\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Selecting best performing algoritms: CatBoost, XGBoost, MLP Classifier and Random Forest for hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for best performing classifiers with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(f\"Random Forest best parameters: {rf_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [200, 300, 400]\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(MLPClassifier(random_state=42), mlp_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "mlp_grid.fit(X_train, y_train)\n",
    "print(f\"MLP Classifier best parameters: {mlp_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(XGBClassifier(eval_metric='mlogloss', random_state=42), xgb_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "print(f\"XGBoost best parameters: {xgb_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "cat_params = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'depth': [3, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "cat_grid = GridSearchCV(CatBoostClassifier(verbose=0, random_state=42), cat_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "cat_grid.fit(X_train, y_train)\n",
    "print(f\"CatBoost best parameters: {cat_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best models on the test set\n",
    "best_rf = rf_grid.best_estimator_\n",
    "best_mlp = mlp_grid.best_estimator_\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "best_cat = cat_grid.best_estimator_\n",
    "\n",
    "for name, clf in [('Random Forest', best_rf), ('MLP Classifier', best_mlp), ('XGBoost', best_xgb), ('CatBoost', best_cat)]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for best performing classifiers with Hyperopt (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: name 'X_train' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m classifier_name, space \u001b[39min\u001b[39;00m [(\u001b[39m'\u001b[39m\u001b[39mRandomForest\u001b[39m\u001b[39m'\u001b[39m, space_rf), (\u001b[39m'\u001b[39m\u001b[39mMLPClassifier\u001b[39m\u001b[39m'\u001b[39m, space_mlp), (\u001b[39m'\u001b[39m\u001b[39mXGBoost\u001b[39m\u001b[39m'\u001b[39m, space_xgb), (\u001b[39m'\u001b[39m\u001b[39mCatBoost\u001b[39m\u001b[39m'\u001b[39m, space_cat)]:\n\u001b[1;32m     83\u001b[0m     trials \u001b[39m=\u001b[39m Trials()\n\u001b[0;32m---> 84\u001b[0m     best \u001b[39m=\u001b[39m fmin(\u001b[39mlambda\u001b[39;49;00m args: objective(args, classifier_name), space, algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest, max_evals\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, trials\u001b[39m=\u001b[39;49mtrials)\n\u001b[1;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mclassifier_name\u001b[39m}\u001b[39;00m\u001b[39m best parameters: \u001b[39m\u001b[39m{\u001b[39;00mbest\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[1;32m    541\u001b[0m         fn,\n\u001b[1;32m    542\u001b[0m         space,\n\u001b[1;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[1;32m    672\u001b[0m     fn,\n\u001b[1;32m    673\u001b[0m     space,\n\u001b[1;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    689\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[18], line 84\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m classifier_name, space \u001b[39min\u001b[39;00m [(\u001b[39m'\u001b[39m\u001b[39mRandomForest\u001b[39m\u001b[39m'\u001b[39m, space_rf), (\u001b[39m'\u001b[39m\u001b[39mMLPClassifier\u001b[39m\u001b[39m'\u001b[39m, space_mlp), (\u001b[39m'\u001b[39m\u001b[39mXGBoost\u001b[39m\u001b[39m'\u001b[39m, space_xgb), (\u001b[39m'\u001b[39m\u001b[39mCatBoost\u001b[39m\u001b[39m'\u001b[39m, space_cat)]:\n\u001b[1;32m     83\u001b[0m     trials \u001b[39m=\u001b[39m Trials()\n\u001b[0;32m---> 84\u001b[0m     best \u001b[39m=\u001b[39m fmin(\u001b[39mlambda\u001b[39;00m args: objective(args, classifier_name), space, algo\u001b[39m=\u001b[39mtpe\u001b[39m.\u001b[39msuggest, max_evals\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, trials\u001b[39m=\u001b[39mtrials)\n\u001b[1;32m     85\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mclassifier_name\u001b[39m}\u001b[39;00m\u001b[39m best parameters: \u001b[39m\u001b[39m{\u001b[39;00mbest\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 45\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(args, classifier_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m     iterations, learning_rate, depth, l2_leaf_reg \u001b[39m=\u001b[39m args\n\u001b[1;32m     36\u001b[0m     clf \u001b[39m=\u001b[39m CatBoostClassifier(\n\u001b[1;32m     37\u001b[0m         iterations\u001b[39m=\u001b[39miterations,\n\u001b[1;32m     38\u001b[0m         learning_rate\u001b[39m=\u001b[39mlearning_rate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[0;32m---> 45\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mmean(cross_val_score(clf, X_train, y_train, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the objective function for optimization\n",
    "def objective(args, classifier_name):\n",
    "    if classifier_name == 'RandomForest':\n",
    "        n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf = args\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            max_features=max_features,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'MLPClassifier':\n",
    "        hidden_layer_sizes, alpha, activation, solver, learning_rate = args\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            alpha=alpha,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        n_estimators, learning_rate, max_depth, gamma, subsample, colsample_bytree = args\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            random_state=42\n",
    "        )    \n",
    "    elif classifier_name == 'CatBoost':\n",
    "        iterations, learning_rate, depth, l2_leaf_reg = args\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=iterations,\n",
    "            learning_rate=learning_rate,\n",
    "            depth=depth,\n",
    "            l2_leaf_reg=l2_leaf_reg,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    score = -np.mean(cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1))\n",
    "    return score\n",
    "\n",
    "# Define extensive hyperparameter search spaces for each classifier\n",
    "space_rf = [\n",
    "    hp.choice('n_estimators', range(10, 201, 10)),\n",
    "    hp.choice('max_depth', list(range(1, 33)) + [None]),\n",
    "    hp.choice('max_features', ['auto', 'sqrt', 'log2', None] + list(np.arange(0.1, 1.1, 0.1))),\n",
    "    hp.choice('min_samples_split', range(2, 21)),\n",
    "    hp.choice('min_samples_leaf', range(1, 21))\n",
    "]\n",
    "\n",
    "space_mlp = [\n",
    "    hp.choice('hidden_layer_sizes', [(i,) for i in range(10, 101, 10)] + [(i, i) for i in range(10, 101, 10)]),\n",
    "    hp.loguniform('alpha', -5, -1),\n",
    "    hp.choice('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "    hp.choice('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "    hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "]\n",
    "\n",
    "space_xgb = [\n",
    "    hp.choice('n_estimators', range(10, 201, 10)),\n",
    "    hp.loguniform('learning_rate', -5, 0),\n",
    "    hp.choice('max_depth', list(range(1, 33))),\n",
    "    hp.loguniform('gamma', -5, 0),\n",
    "    hp.uniform('subsample', 0.1, 1),\n",
    "    hp.uniform('colsample_bytree', 0.1, 1)\n",
    "]\n",
    "\n",
    "space_cat = [\n",
    "    hp.choice('iterations', range(10, 201, 10)),\n",
    "    hp.loguniform('learning_rate', -5, 0),\n",
    "    hp.choice('depth', list(range(1, 17))),\n",
    "    hp.loguniform('l2_leaf_reg', 0, 5)\n",
    "]\n",
    "\n",
    "# Optimize hyperparameters for each classifier\n",
    "for classifier_name, space in [('RandomForest', space_rf), ('MLPClassifier', space_mlp), ('XGBoost', space_xgb), ('CatBoost', space_cat)]:\n",
    "    trials = Trials()\n",
    "    best = fmin(lambda args: objective(args, classifier_name), space, algo=tpe.suggest, max_evals=200, trials=trials)\n",
    "    print(f\"{classifier_name} best parameters: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found out in Google Colab through Hyperopt\n",
    "best_rf = {\n",
    "    'max_depth': 27, \n",
    "    'max_features': 0.30000000000000004, \n",
    "    'min_samples_leaf': 1, \n",
    "    'min_samples_split': 13, \n",
    "    'n_estimators': 90\n",
    "    }\n",
    "\n",
    "best_mlp = {\n",
    "    'activation': 'relu',\n",
    "    'alpha': 0.059008349443448974,\n",
    "    'hidden_layer_sizes': (30, 30),\n",
    "    'learning_rate': 'invscaling',\n",
    "    'solver': 'adam'\n",
    "}\n",
    "\n",
    "best_xgb = {\n",
    "    'colsample_bytree': 0.9334254355105551,\n",
    "    'gamma': 0.008801002728149786,\n",
    "    'learning_rate': 0.1257056414560802,\n",
    "    'max_depth': 9,\n",
    "    'n_estimators': 170,\n",
    "    'subsample': 0.9801605605745425\n",
    "}\n",
    "\n",
    "best_cat = {\n",
    "    'l2_leaf_reg': 3.9355845098832787,\n",
    "    'learning_rate': 0.12846062021329857,\n",
    "    'depth': 9,\n",
    "    'iterations': 17\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(classifier_name, best_params):\n",
    "    if classifier_name == 'RandomForest':\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'] if best_params['max_depth'] != 32 else None,\n",
    "            max_features=best_params['max_features'],\n",
    "            min_samples_split=best_params['min_samples_split'],\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'MLPClassifier':\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "            alpha=best_params['alpha'],\n",
    "            activation=best_params['activation'],\n",
    "            solver=best_params['solver'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            gamma=best_params['gamma'],\n",
    "            subsample=best_params['subsample'],\n",
    "            colsample_bytree=best_params['colsample_bytree'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'CatBoost':\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=best_params['iterations'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            depth=best_params['depth'],\n",
    "            l2_leaf_reg=best_params['l2_leaf_reg'],\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = final_test_df.copy()\n",
    "\n",
    "X_train = final_df.copy()\n",
    "y_train = transformed_df['Overall_Experience'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best parameters for each classifier in a dictionary\n",
    "best_params_dict = {\n",
    "    'RandomForest': best_rf,\n",
    "    'MLPClassifier': best_mlp,\n",
    "    'XGBoost': best_xgb,\n",
    "    'CatBoost': best_cat\n",
    "}\n",
    "\n",
    "# Store results\n",
    "result = {}\n",
    "\n",
    "# Evaluate test set accuracy with the best parameters for each classifier\n",
    "for classifier_name in best_params_dict:\n",
    "    # Build the classifier with the best parameters\n",
    "    clf = build_classifier(classifier_name, best_params_dict[classifier_name])\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    result[classifier_name] = y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['CatBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final.to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
