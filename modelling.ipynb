{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOrdinalEncoder:\n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "        self.cat_to_int = {}\n",
    "        self.int_to_cat = {}\n",
    "        for i, cat in enumerate(self.categories):\n",
    "            self.cat_to_int[cat] = i\n",
    "            self.int_to_cat[i] = cat\n",
    "\n",
    "    def transform(self, data):\n",
    "        return np.array([self.cat_to_int[cat] if cat in self.cat_to_int else np.nan for cat in data])\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return np.array([self.int_to_cat[int(cat)] for cat in data])\n",
    "\n",
    "def encode_ordinal_columns(df, ordinal_columns, n_classes):\n",
    "    encoders = {}\n",
    "    encoded_df = df.copy()\n",
    "    for col in ordinal_columns:\n",
    "        unique_values = sorted(df[col].dropna().unique())\n",
    "        categories = unique_values + [f\"extra_class_{i}\" for i in range(n_classes - len(unique_values))]\n",
    "        encoder = CustomOrdinalEncoder(categories)\n",
    "        encoded_df[col] = encoder.transform(df[col])\n",
    "        encoders[col] = encoder\n",
    "    return encoded_df, encoders\n",
    "\n",
    "def impute_missing_ordinal_records(df, ordinal_columns, n_classes=5, max_iter=10, random_state=42):\n",
    "    encoded_df, encoders = encode_ordinal_columns(df, ordinal_columns, n_classes)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=max_iter, estimator=RandomForestRegressor(random_state=random_state), random_state=random_state)\n",
    "    imputed_array = imputer.fit_transform(encoded_df)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "    imputed_df[ordinal_columns] = np.round(imputed_df[ordinal_columns])\n",
    "\n",
    "    for col in ordinal_columns:\n",
    "        imputed_df[col] = encoders[col].inverse_transform(imputed_df[col])\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "def encode_non_ordinal_columns(df, non_ordinal_columns):\n",
    "    encoded_df = pd.get_dummies(df, columns=non_ordinal_columns, drop_first=True)\n",
    "    return encoded_df\n",
    "\n",
    "def impute_missing_non_ordinal_records(df, max_iter=10, random_state=42):\n",
    "    imputer = IterativeImputer(max_iter=max_iter, estimator=RandomForestRegressor(random_state=random_state), random_state=random_state)\n",
    "    imputed_array = imputer.fit_transform(df)\n",
    "\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "    return imputed_df\n",
    "\n",
    "def impute_most_common(df):\n",
    "    for column in df.columns:\n",
    "        most_common_value = df[column].mode()[0]\n",
    "        df[column].fillna(most_common_value, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv('Surveydata_train.csv', )\n",
    "survey_df_test = pd.read_csv('Surveydata_test.csv')\n",
    "\n",
    "travel_df = pd.read_csv('Traveldata_train.csv')\n",
    "travel_df_test = pd.read_csv('Traveldata_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(survey_df, travel_df, on= 'ID')\n",
    "merged_df_test = pd.merge(survey_df_test, travel_df_test, on= 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = (\n",
    "    merged_df\n",
    "    # 'Seat_comfort', 'Arrival_time_convenient', 'Catering', 'Onboardwifi_service', 'Onboard_entertainment', 'Online_support',\n",
    "    # 'Onlinebooking_Ease', 'Onboard_service', 'Leg_room', 'Checkin_service', 'Cleanliness', 'Online_boarding'\n",
    "    .replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'], [5, 4, 3, 2, 1, 0])\n",
    "    # Platform_location\n",
    "    .replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'], [5, 4, 3, 2, 1, 0])\n",
    "    # Seat_Class\n",
    "    .replace(['Ordinary', 'Green Car'], [0, 1])\n",
    "    # Gender\n",
    "    .replace(['Male', 'Female'], [0, 1])\n",
    "    # CustomerType\n",
    "    .replace(['Disloyal Customer', 'Loyal Customer'], [0, 1])\n",
    "    # TypeTravel\n",
    "    .replace(['Personal Travel', 'Business Travel'], [0, 1])\n",
    "    # Travel_Class\n",
    "    .replace(['Eco', 'Business'], [0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_df = (\n",
    "    merged_df_test\n",
    "    .replace(['Excellent', 'Good', 'Acceptable', 'Needs Improvement', 'Poor', 'Extremely Poor'], [5, 4, 3, 2, 1, 0])\n",
    "    .replace(['Very Convenient', 'Convenient', 'Manageable', 'Needs Improvement', 'Inconvenient', 'Very Inconvenient'], [5, 4, 3, 2, 1, 0])\n",
    "    .replace(['Ordinary', 'Green Car'], [0, 1])\n",
    "    .replace(['Male', 'Female'], [0, 1])\n",
    "    .replace(['Disloyal Customer', 'Loyal Customer'], [0, 1])\n",
    "    .replace(['Personal Travel', 'Business Travel'], [0, 1])\n",
    "    .replace(['Eco', 'Business'], [0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df['Age'] = pd.cut(transformed_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])\n",
    "transformed_test_df['Age'] = pd.cut(transformed_test_df['Age'], 5, labels = ['25', '35', '45', '60', '80'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns = [\n",
    "    'Seat_Comfort', 'Onboard_Wifi_Service', \n",
    "    'Onboard_Entertainment', 'Online_Support', 'Ease_of_Online_Booking', 'Onboard_Service', \n",
    "    'Legroom', 'Baggage_Handling', 'CheckIn_Service', 'Cleanliness', 'Online_Boarding'\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'Customer_Type', 'Travel_Class'\n",
    "  ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal_imputed = pd.read_csv('Ordinal_Imputed.csv')\n",
    "ordinal_imputed = pd.read_csv('Ordinal_Imputed_Full_BaysianRidge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_non_ordinal_df = encode_non_ordinal_columns(transformed_df[categorical_columns], categorical_columns)\n",
    "# ordinal_imputed['ID'] = transformed_df['ID']\n",
    "# encoded_non_ordinal_df['ID'] = transformed_df['ID']\n",
    "\n",
    "# encoded_df = pd.merge(encoded_non_ordinal_df, ordinal_imputed, on= 'ID')\n",
    "\n",
    "# categorical_imputed = pd.read_csv('Categorical_Imputed.csv')\n",
    "categorical_imputed = pd.read_csv('Categorical_Imputed_Full_BaysianRidge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = encode_non_ordinal_columns(categorical_imputed.copy().drop(['ID'], axis=1), ordinal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_df.info())\n",
    "display(len(final_df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal_test_imputed = pd.read_csv('Ordinal_Test_Imputed.csv')\n",
    "ordinal_test_imputed = pd.read_csv('Ordinal_Test_Imputed_Full_BaysianRidge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_test_non_ordinal_df = encode_non_ordinal_columns(transformed_test_df[categorical_columns], categorical_columns)\n",
    "# ordinal_test_imputed['ID'] = transformed_test_df['ID']\n",
    "# encoded_test_non_ordinal_df['ID'] = transformed_test_df['ID']\n",
    "\n",
    "# encoded_test_df = pd.merge(encoded_test_non_ordinal_df, ordinal_test_imputed, on= 'ID')\n",
    "\n",
    "# categorical_test_imputed = pd.read_csv('Categorical_Test_Imputed.csv')\n",
    "categorical_test_imputed = pd.read_csv('Categorical_Test_Imputed_Full_BaysianRidge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df = (\n",
    "    encode_non_ordinal_columns(categorical_test_imputed.copy().drop(['ID'], axis=1), ordinal_columns)\n",
    ")\n",
    "\n",
    "final_test_df['CheckIn_Service_1.0'] = 0\n",
    "final_test_df['Cleanliness_1.0'] = 0\n",
    "final_test_df['Onboard_Service_1.0'] = 0\n",
    "final_test_df['Online_Support_1.0'] = 0\n",
    "final_test_df['Platform_Location_1.0'] = 0\n",
    "\n",
    "final_test_df = final_test_df[final_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_test_df.info())\n",
    "display(len(final_test_df.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Classifier Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.copy()\n",
    "y = transformed_df['Overall_Experience'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifiers = [\n",
    "     ('Logistic Regression', LogisticRegression(solver='liblinear')),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100)),\n",
    "    ('Support Vector Machine', SVC(kernel='linear', C=1)),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('MLP Classifier', MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)),\n",
    "    ('Stochastic Gradient Descent', SGDClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(eval_metric='mlogloss', random_state=42)),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42)),\n",
    "    ('CatBoost', CatBoostClassifier(verbose=0, random_state=42)),\n",
    "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Ridge', RidgeClassifier(random_state=42)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('HistGradientBoostingClassifier', HistGradientBoostingClassifier(random_state=42))\n",
    "\n",
    "]\n",
    "\n",
    "# Iterate through the classifiers, fit, and print accuracy\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Selecting best performing algoritms: CatBoost, XGBoost, MLP Classifier and Random Forest for hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for best performing classifiers with GridSearchCV & RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(f\"Random Forest best parameters: {rf_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Classifier\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [200, 300, 400]\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(MLPClassifier(random_state=42), mlp_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "mlp_grid.fit(X_train, y_train)\n",
    "print(f\"MLP Classifier best parameters: {mlp_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(XGBClassifier(eval_metric='mlogloss', random_state=42), xgb_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "print(f\"XGBoost best parameters: {xgb_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "cat_params = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'depth': [3, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "cat_grid = GridSearchCV(CatBoostClassifier(verbose=0, random_state=42), cat_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "cat_grid.fit(X_train, y_train)\n",
    "print(f\"CatBoost best parameters: {cat_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HistGradientBoost\n",
    "hist_params = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_iter': [100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    'max_leaf_nodes': [31, 35, 40],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_leaf': [20, 25, 30],\n",
    "    'max_bins': [5, 10, 20, 40],\n",
    "    'l2_regularization': [0.1, 0.001, 0.0001]\n",
    "}\n",
    "hist_grid = RandomizedSearchCV(HistGradientBoostingClassifier(verbose=0, random_state=42), hist_params, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "hist_grid.fit(X_train, y_train)\n",
    "print(f\"HistGradientBoostingClassifier best parameters: {hist_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best models on the test set\n",
    "best_rf = rf_grid.best_estimator_\n",
    "best_mlp = mlp_grid.best_estimator_\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "best_cat = cat_grid.best_estimator_\n",
    "best_hist = hist_grid.best_estimator_\n",
    "\n",
    "for name, clf in [('Random Forest', best_rf), ('MLP Classifier', best_mlp), ('XGBoost', best_xgb), ('CatBoost', best_cat), ('HistGradBoost', hist_grid)]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning for best performing classifiers with Hyperopt (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for optimization\n",
    "def objective(args, classifier_name):\n",
    "    if classifier_name == 'RandomForest':\n",
    "        n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf = args\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            max_features=max_features,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'MLPClassifier':\n",
    "        hidden_layer_sizes, alpha, activation, solver, learning_rate = args\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            alpha=alpha,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        n_estimators, learning_rate, max_depth, gamma, subsample, colsample_bytree, tree_method = args\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            tree_method=tree_method,\n",
    "            random_state=42\n",
    "        ) \n",
    "    elif classifier_name == 'CatBoost':\n",
    "        iterations, learning_rate, depth, l2_leaf_reg = args\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=iterations,\n",
    "            learning_rate=learning_rate,\n",
    "            depth=depth,\n",
    "            l2_leaf_reg=l2_leaf_reg,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    elif classifier_name == 'HistGradBoost':\n",
    "        min_samples_leaf, max_leaf_nodes, max_iter, max_depth, max_bins, learning_rate, l2_regularization = args\n",
    "        clf = HistGradientBoostingClassifier(\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            max_iter=max_iter,\n",
    "            max_depth=max_depth,\n",
    "            max_bins=max_bins,\n",
    "            learning_rate=learning_rate,\n",
    "            l2_regularization=l2_regularization,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    score = -np.mean(cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1))\n",
    "    return score\n",
    "\n",
    "# Define extensive hyperparameter search spaces for each classifier\n",
    "space_rf = [\n",
    "    hp.choice('n_estimators', range(10, 201, 10)),\n",
    "    hp.choice('max_depth', list(range(1, 33)) + [None]),\n",
    "    hp.choice('max_features', ['auto', 'sqrt', 'log2', None] + list(np.arange(0.1, 1.1, 0.1))),\n",
    "    hp.choice('min_samples_split', range(2, 21)),\n",
    "    hp.choice('min_samples_leaf', range(1, 21))\n",
    "]\n",
    "\n",
    "space_mlp = [\n",
    "    hp.choice('hidden_layer_sizes', [(i,) for i in range(10, 101, 10)] + [(i, i) for i in range(10, 101, 10)]),\n",
    "    hp.loguniform('alpha', -5, -1),\n",
    "    hp.choice('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "    hp.choice('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "    hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "]\n",
    "\n",
    "space_xgb = [\n",
    "    hp.choice('n_estimators', range(10, 201, 10)),\n",
    "    hp.loguniform('learning_rate', -5, 0),\n",
    "    hp.choice('max_depth', list(range(1, 33))),\n",
    "    hp.loguniform('gamma', -5, 0),\n",
    "    hp.uniform('subsample', 0.1, 1),\n",
    "    hp.uniform('colsample_bytree', 0.1, 1)\n",
    "]\n",
    "\n",
    "space_cat = [\n",
    "    hp.choice('iterations', range(10, 201, 10)),\n",
    "    hp.loguniform('learning_rate', -5, 0),\n",
    "    hp.choice('depth', list(range(1, 17))),\n",
    "    hp.loguniform('l2_leaf_reg', 0, 5)\n",
    "]\n",
    "\n",
    "space_hist_gb = {\n",
    "    hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    hp.quniform('max_iter', 50, 500, 1),\n",
    "    hp.quniform('max_leaf_nodes', 15, 255, 1),\n",
    "    hp.quniform('max_depth', 3, 15, 1),\n",
    "    hp.quniform('min_samples_leaf', 5, 50, 1),\n",
    "    hp.loguniform('l2_regularization', np.log(0.0001), np.log(1)),\n",
    "    hp.quniform('max_bins', 50, 255, 1),\n",
    "}\n",
    "\n",
    "# Optimize hyperparameters for each classifier\n",
    "for classifier_name, space in [('RandomForest', space_rf), ('MLPClassifier', space_mlp), ('XGBoost', space_xgb), ('CatBoost', space_cat), ('HistGradBoost', space_hist_gb)]:\n",
    "    trials = Trials()\n",
    "    best = fmin(lambda args: objective(args, classifier_name), space, algo=tpe.suggest, max_evals=200, trials=trials)\n",
    "    print(f\"{classifier_name} best parameters: {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found out in Google Colab through Hyperopt\n",
    "best_rf = {\n",
    "    'max_depth': 27, \n",
    "    'max_features': 6, \n",
    "    'min_samples_leaf': 1, \n",
    "    'min_samples_split': 13, \n",
    "    'n_estimators': 90\n",
    "    }\n",
    "\n",
    "best_mlp = {\n",
    "    'activation': 'relu',\n",
    "    'alpha': 0.059008349443448974,\n",
    "    'hidden_layer_sizes': (30, 30),\n",
    "    'learning_rate': 'invscaling',\n",
    "    'solver': 'adam'\n",
    "}\n",
    "\n",
    "best_xgb = {\n",
    "    'colsample_bytree': 0.9334254355105551,\n",
    "    'gamma': 0.008801002728149786,\n",
    "    'learning_rate': 0.1257056414560802,\n",
    "    'max_depth': 9,\n",
    "    'n_estimators': 170,\n",
    "    'subsample': 0.9801605605745425,\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "best_cat = {\n",
    "    'l2_leaf_reg': 3.9355845098832787,\n",
    "    'learning_rate': 0.12846062021329857,\n",
    "    'depth': 9,\n",
    "    'iterations': 17\n",
    "}\n",
    "\n",
    "best_hist = {\n",
    "    'min_samples_leaf': 30,\n",
    "    'max_leaf_nodes': 31,\n",
    "    'max_iter': 500,\n",
    "    'max_depth': 30,\n",
    "    'max_bins': 10,\n",
    "    'learning_rate': 0.1 ,\n",
    "    'l2_regularization': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(classifier_name, best_params):\n",
    "    if classifier_name == 'RandomForest':\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_depth=best_params['max_depth'] if best_params['max_depth'] != 32 else None,\n",
    "            max_features=best_params['max_features'],\n",
    "            min_samples_split=best_params['min_samples_split'],\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'MLPClassifier':\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "            alpha=best_params['alpha'],\n",
    "            activation=best_params['activation'],\n",
    "            solver=best_params['solver'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'XGBoost':\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            gamma=best_params['gamma'],\n",
    "            subsample=best_params['subsample'],\n",
    "            colsample_bytree=best_params['colsample_bytree'],\n",
    "            tree_method=best_params['tree_method'],\n",
    "            random_state=42\n",
    "        )\n",
    "    elif classifier_name == 'CatBoost':\n",
    "        clf = CatBoostClassifier(\n",
    "            iterations=best_params['iterations'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            depth=best_params['depth'],\n",
    "            l2_leaf_reg=best_params['l2_leaf_reg'],\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    elif classifier_name == 'HistGradBoost':\n",
    "        clf = HistGradientBoostingClassifier(\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            max_leaf_nodes=best_params['max_leaf_nodes'],\n",
    "            max_iter=best_params['max_iter'],\n",
    "            max_depth=best_params['max_depth'],\n",
    "            max_bins=best_params['max_bins'],\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            l2_regularization=best_params['l2_regularization'],\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = final_test_df.copy()\n",
    "\n",
    "X_train = final_df.copy()\n",
    "y_train = transformed_df['Overall_Experience'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best parameters for each classifier in a dictionary\n",
    "best_params_dict = {\n",
    "    'RandomForest': best_rf,\n",
    "    'MLPClassifier': best_mlp,\n",
    "    'XGBoost': best_xgb,\n",
    "    'CatBoost': best_cat,\n",
    "    'HistGradBoost': best_hist\n",
    "}\n",
    "\n",
    "# Store results\n",
    "result = {}\n",
    "\n",
    "# Evaluate test set accuracy with the best parameters for each classifier\n",
    "for classifier_name in best_params_dict:\n",
    "    # Build the classifier with the best parameters\n",
    "    clf = build_classifier(classifier_name, best_params_dict[classifier_name])\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    result[classifier_name] = y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['CatBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final.to_csv('Submission_cat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final_hist = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['HistGradBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final_hist.to_csv('Submission_hist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final_xgboost = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['XGBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final_xgboost.to_csv('Submission_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final_mlp = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['MLPClassifier']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final_mlp.to_csv('Submission_mlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final_rf = pd.DataFrame(data={'ID': range(99900001,99935603), 'Overall_Experience': result['RandomForest']}).set_index('ID').sort_index(ascending=True)\n",
    "result_final_rf.to_csv('Submission_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_final = pd.DataFrame(data={'ID': range(99900001,99935603), \\\n",
    "                                  'Overall_Experience_rf': result['RandomForest'], \\\n",
    "                                  'Overall_Experience_mlp': result['MLPClassifier'], \\\n",
    "                                  'Overall_Experience_xgb': result['XGBoost'], \\\n",
    "                                  'Overall_Experience_cat': result['CatBoost'], \\\n",
    "                                  'Overall_Experience_hist': result['HistGradBoost']}).set_index('ID').sort_index(ascending=True)\n",
    "\n",
    "result_final.to_csv('Submission_combined.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
